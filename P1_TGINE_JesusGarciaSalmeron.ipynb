{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFhzVbnwyd6IGLwnIXqCwM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesusGS01/BDGE/blob/main/P1_TGINE_JesusGarciaSalmeron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica 1 : Extracción y búsqueda de información textual\n",
        "\n",
        "Alumno: Jesús García Salmerón\n",
        "\n",
        "Convocatoria: Enero, 2024"
      ],
      "metadata": {
        "id": "lJi2Y8Wvrowp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parte 1- Compilar datos de documentos web"
      ],
      "metadata": {
        "id": "7o5pfT5Fsv3J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVg5iWm4rjwS",
        "outputId": "01f05045-a700-40a6-e79b-2e317ac6ad59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: Twisted<23.8.0,>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (22.10.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (41.0.5)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.2.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.8.1)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.3.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.6.2)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.1.2)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (6.1)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.3.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.1.1)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.3)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -U scrapy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy\n",
        "import sys\n",
        "import json\n",
        "import locale\n",
        "import time\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Para cada crawler que nos definimos nos debemos crear una clase Spider que debe heredar de la clase scrapy.Spider\n",
        "\n",
        "class UMSpider (scrapy.Spider):\n",
        "    name = 'animals_nationalGeographic'\n",
        "\n",
        "    # Decimos que el dominio válido es el de la UM\n",
        "    allowed_domains = ['www.nationalgeographic.com.es']\n",
        "\n",
        "    # podemos definir las páginas de inicio\n",
        "    start_urls = ['https://www.nationalgeographic.com.es/mundo-animal/tiburones-sobrevivieron-casi-20-anos-lago-campo-golf-como-hicieron_20748']\n",
        "\n",
        "    # para evitar que el sitio te bloquee por usar scrapy es interesante cambiar el USER_AGENT\n",
        "    # El user agent por defecto de Scrapy cuando hace una petición es\n",
        "    # Scrapy/VERSION (+https://scrapy.org)\n",
        "    custom_settings = {\n",
        "        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    }\n",
        "\n",
        "    # debemos de implementar este método que se llamará para cada una de las páginas que se vayan analizando\n",
        "    def parse (self, response):\n",
        "        \"\"\"\n",
        "        @inherit\n",
        "\n",
        "        @param self\n",
        "        @param response\n",
        "        \"\"\"\n",
        "\n",
        "        # Guardamos la URL del sitio que se está visitando\n",
        "        url = str(response.request.url).strip()\n",
        "\n",
        "        # Cogemos el contenido relevante y para eso debemos usar selectores CSS\n",
        "        for article in response.css ('.article-main'):\n",
        "            # Cogemos el contenido del título\n",
        "            title = str (article.css ('h1::text').get ()).strip()\n",
        "            title = BeautifulSoup (title, 'html.parser').get_text().strip()\n",
        "            subtitle = str (article.css ('h2::text').get ()).strip()\n",
        "            subtitle = BeautifulSoup (subtitle, 'html.parser').get_text().strip()\n",
        "\n",
        "            # Cogemos el contenido del contenido\n",
        "            content = ''.join (article.css('.article-text.txt').get())\n",
        "            soup  = BeautifulSoup (content, 'html.parser')\n",
        "\n",
        "            # Lista de clases a eliminar\n",
        "            unwanted_tags = [\"subscription-highlighted\", \"related-article-content\", \"ad-inline\"]\n",
        "\n",
        "            # Buscamos y eliminamos las etiquetas no deseadas\n",
        "            for tag in unwanted_tags:\n",
        "                for el in soup.find_all(class_=tag):\n",
        "                    el.extract()\n",
        "\n",
        "            content = soup.get_text().strip().replace(\"\\\"\",\"\")\n",
        "\n",
        "            # Cogemos también la fecha si existe\n",
        "            date = str(article.css ('.article-top-info time').get().strip())\n",
        "            if (date):\n",
        "                date = BeautifulSoup (date, 'html.parser').get_text().strip()\n",
        "\n",
        "            print (\"-------------------------\")\n",
        "            print (url)\n",
        "            print (title)\n",
        "            print (subtitle)\n",
        "            print (content)\n",
        "            print (date)\n",
        "            print (\"-------------------------\")\n",
        "\n",
        "            data = {\n",
        "                'url' : url,\n",
        "                'title': title,\n",
        "                'subtitle': subtitle\n",
        "                #'content': content,\n",
        "                # 'date': date\n",
        "            }\n",
        "\n",
        "            # filename = str(random.random()).replace(\".\",\"\") + \".json\"\n",
        "\n",
        "            # # Guardamos el documento si tiene contenido y título\n",
        "            # if content and title:\n",
        "            #     with open ('animals/' + filename, 'w') as f:\n",
        "            #         json.dump (data, f, indent = 4)\n",
        "\n",
        "\n",
        "        # Obtenemos todas las otros links de la página representados por la etiqueta <a>\n",
        "        # url_in_current_document = response.css ('a')\n",
        "        # for next_page in url_in_current_document:\n",
        "        #     # Para limitar que solamente se parseen las noticias dentro de 'https://www.um.es/web/sala-prensa/'\n",
        "        #     url_str = str(next_page.css('::attr(href)').get())\n",
        "        #     if \"www.um.es/web/sala-prensa/\" in url_str:\n",
        "        #       # obtenemos el atributo href de la etiqueta <a> y parseamos la página\n",
        "        #       yield response.follow (next_page, self.parse)"
      ],
      "metadata": {
        "id": "cGjbgoXEuDYg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scrapy, os\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "# Creamos un proceso de Crawler podemos poner distintas settings que están definidas en la documentación.\n",
        "# Entre ellas podemos ocular los logs del proceso de Crawling.\n",
        "process = CrawlerProcess(settings={\n",
        "    \"LOG_ENABLED\": False,\n",
        "    # Used for pipeline 1\n",
        "})\n",
        "\n",
        "# Como se ha definido anteriormente en el RSSCrawler, los ficheros se van a almacenar en la carpeta \"rss\"\n",
        "# Comprobamos que existe la carpeta y si no existe la creamos\n",
        "if (os.path.exists('animals')== False):\n",
        "    os.mkdir('animals')\n",
        "\n",
        "# Creamos el proceso con el RSSSpider\n",
        "process.crawl(UMSpider)\n",
        "# Ejecutamos el Crawler\n",
        "process.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1GLL5wWwApe",
        "outputId": "d2d7398d-082f-416a-de31-61a135f9dbf2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:scrapy.utils.log:Scrapy 2.11.0 started (bot: scrapybot)\n",
            "INFO:scrapy.utils.log:Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0], pyOpenSSL 23.3.0 (OpenSSL 3.1.4 24 Oct 2023), cryptography 41.0.5, Platform Linux-5.15.120+-x86_64-with-glibc2.35\n",
            "INFO:scrapy.addons:Enabled addons:\n",
            "[]\n",
            "/usr/local/lib/python3.10/dist-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
            "\n",
            "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
            "\n",
            "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
            "  return cls(crawler)\n",
            "DEBUG:scrapy.utils.log:Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "INFO:scrapy.extensions.telnet:Telnet Password: 2b6b328dfd442354\n",
            "INFO:scrapy.middleware:Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "INFO:scrapy.crawler:Overridden settings:\n",
            "{'LOG_ENABLED': False,\n",
            " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 '\n",
            "               '(KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
            "INFO:scrapy.middleware:Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "INFO:scrapy.middleware:Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "INFO:scrapy.middleware:Enabled item pipelines:\n",
            "[]\n",
            "INFO:scrapy.core.engine:Spider opened\n",
            "INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "INFO:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6023\n",
            "DEBUG:scrapy.core.engine:Crawled (200) <GET https://www.nationalgeographic.com.es/mundo-animal/tiburones-sobrevivieron-casi-20-anos-lago-campo-golf-como-hicieron_20748> (referer: None)\n",
            "INFO:scrapy.core.engine:Closing spider (finished)\n",
            "INFO:scrapy.statscollectors:Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 386,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 39887,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 0.344267,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2023, 11, 29, 14, 34, 59, 717607, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 119033,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 2,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 141783040,\n",
            " 'memusage/startup': 141783040,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2023, 11, 29, 14, 34, 59, 373340, tzinfo=datetime.timezone.utc)}\n",
            "INFO:scrapy.core.engine:Spider closed (finished)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------\n",
            "https://www.nationalgeographic.com.es/mundo-animal/tiburones-sobrevivieron-casi-20-anos-lago-campo-golf-como-hicieron_20748\n",
            "Tiburones sobrevivieron casi 20 años en lago de campo de golf, ¿cómo lo hicieron?\n",
            "Las grandes inundaciones provocadas por la crecida de los ríos Logan y Albert habían arrastrado hasta allí a estos escualos, que no pudieron salir del lago una vez las aguas retrocedieron.\n",
            "En 1996, seis tiburones sarda (Carcharhinus leucas) quedaron atrapados en las aguas de un lago de un campo de golf cerca de Brisbane, en Australia. Las grandes inundaciones provocadas por la crecida de los ríos Logan y Albert habían arrastrado hasta allí a estos escualos, que no pudieron salir del lago una vez las aguas retrocedieron. Allí estuvieron durante casi dos décadas, sobreviviendo gracias al cuidado del personal del parque.Aunque aquel acontecimiento tuvo un gran eco en los medios de comunicación, sus consecuencias biológicas nunca se habían investigado con profundidad hasta la fecha. Ahora una nuevo estudio realizado a partir de la recopilación de datos científicos de aquellos escualos arroja luz sobre la adaptación de los tiburones a hábitats con baja salinidad.El tiburón sarda (Carcharhinus leucas), también llamado cazón o tiburón toro, es conocido por su habilidad para viajar durante grandes distancias y largos períodos de tiempo en masas de agua dulce en regiones tropicales, subtropicales y templadas de todo el mundo. Es realmente eficiente a la hora de adaptar su organismo a entornos con poca salinidad con costes metabólicos mínimos, como prueban las grandes migraciones que lleva a cabo en extensas masas de agua dulce en los ríos Amazonas, Mississippi y Zambeze.Ahora, una nueva investigación llevada a cabo por científicos de la Universidad del Rurh de Bochum, en Alemania, y publicado en la revista Marine and Fishery Science, editada por el Instituto Nacional de Investigación y Desarrollo Pesquero (INIDEP) de Argentina, ha concluido que estos tiburones podrían haberse pasado toda la vida viviendo en estos lagos de baja salinidad. La clave: los riñones y las glándulas rectales Solo algunas especies de elasmobranquios son eurihalinos. Esto es, son capaces de vivir en aguas dulces con altas concentraciones de sales. Lo consiguen gracias a un procedimiento interno llamado osmorregulación con el que logran equilibrar su organismo con el exterior controlando los materiales disueltos en su organismo, algo que logran gracias a la acción de los riñones y las glándulas rectales, órganos que están especialmente adaptados para reciclar y retener la sal en su cuerpo. Los investigadores descubrieron que las glándulas rectales de los ejemplares que pasan largos períodos en agua dulce presentaban diferencias significativas con los de sus parientes que viven siempre en aguas saladas, pues estos órganos dejan de funcionar en entornos poco salinos. La especie más adaptadaLos científicos descubrieron que estos escualos -que han sobrevivido un período de 17 años en ese entorno inhóspito para la mayoría de los tiburones- han probado con creces la alta capacidad de resistencia de una especie habituada a entornos con poca salinidad, como pueden ser lagos sin salida al mar. El hábitat lacustre del campo de golf proporcionó a los tiburones jóvenes un lugar donde crecer sin la amenaza de depredadores, pero tampoco tuvieron la oportunidad de capturar presas más grandes o aparearse en su entorno natural. Como consecuencia de ello, los científicos descubrieron que su ritmo de crecimiento fue considerablemente más lento y tampoco se aparearon, algo que corrobora lo que los científicos sospechaban: que los tiburones evitan reproducirse en aguas con poca salinidad.Aun así, los investigadores concluyeron que estos escualos no tienen prácticamente límites en cuanto a osmorregulación se refiere, lo que demuestra su alta capacidad para sobrevivir en cualquier tipo de entorno.El estudio de casos similares, afirman los científicos, podría servir para obtener más información sobre la compleja biología de esta especie, ya que solo un pequeño número de elasmobranquios posee información tan interesante sobre la adaptación a entornos de baja salinidad como lo hacen estos peces.\n",
            "29 de noviembre de 2023, 02:00\n",
            "-------------------------\n"
          ]
        }
      ]
    }
  ]
}